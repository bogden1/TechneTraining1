{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Techne ML workbook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xmeGWAA8o1Li",
        "qkPK2J2FMQn-"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogden1/TechneTraining1/blob/main/Code/Techne_ML_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7bE_8PPoV5k"
      },
      "source": [
        "# Set up variables and install useful library code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkwzM1cf5U0-"
      },
      "source": [
        "import sys\n",
        "data_source = \"Github\"\n",
        "!git clone https://github.com/bogden1/TechneTraining1.git\n",
        "sys.path.insert(0, 'TechneTraining1')\n",
        "sys.path.insert(0, 'TechneTraining1/Code')\n",
        "github_data = \"TechneTraining1/Data/TopicModelling/\"\n",
        "import techne_library_code as tlc\n",
        "from IPython.display import display\n",
        "import math\n",
        "TEST_MODE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwpRhTIvlXGI"
      },
      "source": [
        "# Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmeGWAA8o1Li"
      },
      "source": [
        "### Load word list from Topic Model built on 'regulation' related websites.\n",
        "Display words in table.\n",
        "\n",
        "A topic model is created from a corpus of text by an unsupervised machine learning algorithm. The process is non-deterministic which means the results will differ every time it is run. Below is on results from running the software MALLET over the 'regulation' corpus.\n",
        "\n",
        "The primary output is a list of topics (in this case 8, one row each) and a list of words most representative of that topic. A word can appear in more than one topic.\n",
        "\n",
        "From this we can get a high level overview of a corpus of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8ex1oI6pyO"
      },
      "source": [
        "topic_words = tlc.read_topic_list(github_data + \"topic_list.txt\")\n",
        "TOPICS = len(topic_words)\n",
        "topic_table = tlc.pd.DataFrame([v[0:12] for v in topic_words.values()])\n",
        "topic_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtBZaBd4wSgX"
      },
      "source": [
        "Stop words are used in Natural Language Processing to filter out very common words, or those which may negatively affect the results.\n",
        "\n",
        "Below is a list of example stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Ict_D76URX"
      },
      "source": [
        "stop_words = [\"i\",\"or\",\"which\",\"of\",\"and\",\"is\",\"the\",\"a\",\"you're\",\"you\",\"at\",\"his\",\"etc\",'an','where','when']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qvSbu5wpZ3"
      },
      "source": [
        "Exercise: Picking stop words\n",
        "\n",
        "We can add more to this list by selecting from the following list. Which ones do you think might be worth filtering out?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Pfvyjn9pWV"
      },
      "source": [
        "additional_stops = ['medical','freight','pdf','plan','kb','regulation','risk']\n",
        "stop_word_select = tlc.widgets.SelectMultiple(options=additional_stops, rows=len(additional_stops))\n",
        "stop_word_select"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMwV5ZV8n4NT"
      },
      "source": [
        "for w in stop_word_select.value:\n",
        "    print(\"Adding\",w,\"to stop words\")\n",
        "    stop_words.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQt0SHFVw5tU"
      },
      "source": [
        "As well as a list of topics and related words, MALLET also produces a topic breakdown for each document in the corpus.\n",
        "\n",
        "Here we load the topic data and visualise some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTyOifKFBTzS"
      },
      "source": [
        "topics_per_doc = tlc.read_doc_topics(github_data + \"topics_per_doc.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M_WaCclxQfe"
      },
      "source": [
        "The following plots show the proportion of each topic attributed to 4 different documents.\n",
        "\n",
        "\n",
        "\n",
        "1.   Top Left: One topic clearly dominates\n",
        "2.   Top Right: One dominant topic but a second topic is above the level of others\n",
        "3.   Bottom Left: Two topics clearly above others\n",
        "4.   Bottom Right: Topics close to being even"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixqerZiB0hG"
      },
      "source": [
        "file_number_list = [212, 85, 9, 372]\n",
        "fig, ax = tlc.plot_doc_topics(file_number_list, topics_per_doc, TOPICS)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27umRRru2RXo"
      },
      "source": [
        "### Exercise: From Topics to Classes\n",
        "\n",
        "For this Machine Learning exercise we want to predict a Category of regulation (e.g. \"Medicine\" or \"Rail\"). The categories we may want to predict do not map one-to-one with the topics above. So first we need to create that mapping.\n",
        "\n",
        "Firstly we will define a list of possible categories. Sometimes the topics that come out may be worth ignoring (e.g. cookie information) but in this case all of them seem to be of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U-KX4MGEWEV"
      },
      "source": [
        "topics_of_interest = [0,1,2,3,4,5,6,7]\n",
        "class_names = {0:\"General\",1:\"Medicine\",2:\"Rail\",3:\"Safety\",4:\"Pensions\",5:\"Education\",6:\"Other\",-1:\"Unclassified\"}\n",
        "topic_to_class = {}\n",
        "if TEST_MODE:\n",
        "    topic_to_class = {0:1,1:0,2:2,3:3,4:4,5:4,6:2,7:2}  #For testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6QYWQ9s2_cE"
      },
      "source": [
        "Using the dropdown and list selector below we can set the mapping from topic to Class (a term more commonly used in machine learning for category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWRIL6YhwWy"
      },
      "source": [
        "class_drop = tlc.widgets.Dropdown(options=[(v,k) for k,v in class_names.items()], \n",
        "                                  value=topics_of_interest[0], rows = len(class_names))\n",
        "topic_select = tlc.widgets.SelectMultiple(options=[(w[0:5],t) for t,w in topic_words.items()],\n",
        "                                      value = [k for k,v in topic_to_class.items() if v == class_drop.value],\n",
        "                                      rows = TOPICS+1, height=\"100%\")\n",
        "\n",
        "button = tlc.widgets.Button(description=\"Update\")\n",
        "output = tlc.widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    for v in topic_select.value:\n",
        "        topic_to_class[v] = class_drop.value\n",
        "        print(\"Updated\")\n",
        "\n",
        "button.on_click(on_button_clicked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNLKUfw6-p8B"
      },
      "source": [
        "### Exercise: Map topics to classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eojvFzQArUtm"
      },
      "source": [
        "V = tlc.widgets.VBox([class_drop, topic_select, button])\n",
        "V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHbuMTQA3Tna"
      },
      "source": [
        "Update the selected values here and then return to the dropdown until finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqcvHHmx389I"
      },
      "source": [
        "Display the resulting mappings from **Topic** to **Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOjQl1lkGFV"
      },
      "source": [
        "tlc.pd.DataFrame([(\",\".join(topic_words[k][0:10]),class_names[v]) for k,v in topic_to_class.items()], columns=['Topic Words','Class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn33v16g5UmG"
      },
      "source": [
        "## Viewing document class proportions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJofXEU4CNmV"
      },
      "source": [
        "classes_per_doc = tlc.topic_to_class_scores(topics_per_doc, topic_to_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUn4IVn1yjRU"
      },
      "source": [
        "Generally every document contains a bit of each topic. Before visualising the class breakdown for our sample documents, we can filter out lowest scoring classes and focus on the primary class(es) by zeroing all values below a threshold. We then **normalise** the probabilities to add to 1.\n",
        "\n",
        "Run the next piece of code to create a slider to set the threshold, and then the following one will draw graphs. To try a different threshold, adjust the slider and rerun the graph code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQHy9DSofo9i"
      },
      "source": [
        "class_threshold = tlc.widgets.FloatSlider(0.10, min=0.10, max=0.65, step=0.05)\n",
        "class_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-R1Rzx1NQ8E"
      },
      "source": [
        "This is the graph code. It shows Classes above the threshold defined by the slider for the four documents previously visualised. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAf_o1deYGFE"
      },
      "source": [
        "filtered_classes_per_doc = tlc.filter_topics_by_threshold(classes_per_doc, class_threshold.value)\n",
        "class_count = len(filtered_classes_per_doc['file_1.txt'])\n",
        "fig, ax = tlc.plot_doc_topics(file_number_list, filtered_classes_per_doc, class_count, normalise=True)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkPK2J2FMQn-"
      },
      "source": [
        "# Representing text as numbers\n",
        "\n",
        "The next stage is to use the results of the topic modelling to train a Supervised Learning algorithm.\n",
        "\n",
        "Supervised Learning is learning by example. We label our data in advance with categories, and then the algorithm derives a function which will map an input data item to an output Class. The input data is usually termed **Features**, the outputs are often called **Responses**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCMp_hOEmLNs"
      },
      "source": [
        "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "For this exercise the features are the words in our documents. There is no semantic meaning attached, the words are no more than tokens. Imagine a spreadsheet where each row represents a document and each column represents a word from a fixed vocabulary.\n",
        "\n",
        "One representation we could use would be to use a 1 to indicate a word appears in a document, and a 0 if it doesn't. This is simple but overly so. A better representation is TF-IDF which stands for Term Frequency-Inverse Document Frequency. It is a very simple idea but the general gist is that a word that appears in most documents will score lowly, while a word which appears in few documents will score highly (this is the Inverse Document Frequency part). The Term Frequency increases the score when it appears many times in the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBFtHK7LtQ6H"
      },
      "source": [
        "Now that we've mapped topics and categories, it is time to prepare the text corpus (the document contents) for Machine Learning.\n",
        "\n",
        "First we load the content from a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVzM6PH2Qfre"
      },
      "source": [
        "D4ML = tlc.MLData()\n",
        "D4ML.load_content(github_data + \"tm_file_contents.txt\")\n",
        "D4ML.set_classes(classes_per_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1x_8PX0ARl"
      },
      "source": [
        "We have some influence over the parameters used to define the TFIDF representation of our corpus. How they are set can influence the results.\n",
        "\n",
        "1. Features: how many distinct words from the corpus to use for the Vocabulary\n",
        "2. Min Doc Frequency: minimum number of documents a word must appear in to be considered\n",
        "3. Max Doc Frequency: maximum number of documents a word must appear in to be considered\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OQfouoH6IWh"
      },
      "source": [
        "FEATURES=1000\n",
        "MIN_DOC_FREQ=4\n",
        "MAX_DOC_FREQ=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfC8KwxHmrnH"
      },
      "source": [
        "Calculate the TF-IDF scores for each document and prepare some of the data for Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "596ZHrjeMIuV"
      },
      "source": [
        "D4ML.add_stop_words(*stop_words)\n",
        "D4ML.calc_tfidf(FEATURES, MIN_DOC_FREQ, MAX_DOC_FREQ)\n",
        "print(\"Documents:\",D4ML.TFIDF.shape[0],\"\\tWords\",D4ML.TFIDF.shape[1])\n",
        "training_features, training_classes, training_ids = D4ML.get_ml_data()\n",
        "training_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6CY5zMniwq"
      },
      "source": [
        "Here we can see an example of the TF-IDF scores for a document. They are sorted in score order, the higher scores indicating a greater importance for that document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzWDC92fO4AT"
      },
      "source": [
        "EXAMPLE = 0   # 0 to 3 only\n",
        "vocabulary = D4ML.vectorizer.get_feature_names()\n",
        "example_row = D4ML.TFIDF[D4ML.file_to_idx['file_' + str(file_number_list[EXAMPLE]) + '.txt']]\n",
        "example_table = tlc.pd.DataFrame(zip([vocabulary[w] for w in example_row.nonzero()[1]],\n",
        "                                 [int(example_row[0,v]*1000)/1000 for i,v in enumerate(example_row.nonzero()[1])]),\n",
        "                                 columns = ['word','tfidf']).sort_values(by='tfidf', ascending=False)\n",
        "example_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4ctvijlkBG"
      },
      "source": [
        "# Supervised Machine Learning\n",
        "\n",
        "For this exercise we will use the Naive Bayes algorithm. It is called Bayes because it uses Bayesian probability (named after the Reverend Thomas Bayes who discovered it). It is Naive because it assumes that all words in a document are independent of each other (think of the sentence \"my cat miaows when hungry\"). It seems like a bad assumption but actually works well in practice.\n",
        "\n",
        "Bayesian probability is surprisingly simple and gives us the ability to flip probabilities around. From a corpus of text I can easily calculate the probability of the word \"Passenger\" appearing in a document about \"Railways\", and also in a document about \"Medicines\". What Bayes' rule does is allow me to then calculate the probability that a document is about \"Railways\" or \"Medicines\" given that it contains the word \"Passenger\". Very handy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGlymMDBn8Yv"
      },
      "source": [
        "### Preparing training and test data\n",
        "\n",
        "Before starting any Machine Learning we need to split our data into Training and Test datasets. The reason for this is that algorithms can appear to perform very well against the dataset they were trained with, but then perform very badly on new, unseen, data.\n",
        "\n",
        "The algorithm will learn from the Training data and then we check its performance against the Test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhlX9S2veMqJ"
      },
      "source": [
        "TEST_PROPORTION = 0.6\n",
        "X_train, X_test, \\\n",
        "y_train, y_test, \\\n",
        "x_train_ids, x_test_ids = tlc.train_test_split(training_features, training_classes, training_ids,\n",
        "                                               test_size = TEST_PROPORTION, \n",
        "                                               random_state=42, stratify=training_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXa4FQ09p_Xd"
      },
      "source": [
        "### Training the Naive Bayes model\n",
        "\n",
        "Now we **fit** a Naive Bayes model to the training data. Two lines of code and it is done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6R9i3LafxH2"
      },
      "source": [
        "model = tlc.BernoulliNB()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PScAPnQm3Y_Z"
      },
      "source": [
        "### Optional: Under the hood of Naive Bayes\n",
        "One nice feature of Naive Bayes is we can see exactly what is going on internally and could recreate its results with a calculator, if we so wished.\n",
        "\n",
        "The first part of the calculation is called the prior and is the probability of each class without any further information (i.e. without seeing the document content)\n",
        "\n",
        "This corpus is heavily skewed so rail is dominant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEeRBBKcuB6x"
      },
      "source": [
        "tlc.pyplot.bar(height=[math.exp(p) for p in model.class_log_prior_], \n",
        "               x=[x for x in range(len(model.class_log_prior_))])\n",
        "tlc.pyplot.xticks([0,1,2,3,4],[class_names[c] for c in range(len(model.class_log_prior_))])\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adOw9hbr4J6Y"
      },
      "source": [
        "We can also take a class and see which words have the highest probability within that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHs3081eu7qr"
      },
      "source": [
        "C = 1\n",
        "N = 10\n",
        "print(\"Class:\",class_names[C])\n",
        "topN = (-model.feature_log_prob_[C]).argsort()[0:N]\n",
        "words = [vocabulary[w] for w in topN]\n",
        "probs = tlc.np.exp(model.feature_log_prob_[C,topN])\n",
        "tlc.pyplot.bar(height=probs, \n",
        "               x=[x for x in range(len(words))])\n",
        "tlc.pyplot.xticks(range(len(words)),words, rotation=45)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDkkWHEC4UmO"
      },
      "source": [
        "We can then choose a word and see the probability of each class for that word. The final result is a combination of this and the prior.\n",
        "\n",
        "Since the prior is heavily in favour of one class, the word probabilities need to be strongly in favour of another to change the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN42LWLs1FG0"
      },
      "source": [
        "W = 9\n",
        "print(\"Word:\",vocabulary[topN[W]])\n",
        "w_probs = tlc.np.exp([model.feature_log_prob_[i,topN[W]] for i in range(len(model.class_log_prior_))])\n",
        "tlc.pyplot.bar(height=w_probs, \n",
        "               x=[x for x in range(len(model.class_log_prior_))])\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37DBRPqgx1iQ"
      },
      "source": [
        "### Evaluating the model's performance\n",
        "\n",
        "Having created the model we now use it to generate predictions for the test dataset.\n",
        "\n",
        "We have two ways of assessing the performance of the model. The first is to give an accuracy score, quite simply the percentage of predictions it has got right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLiJ5d8Gnkia"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)\n",
        "print(\"Prediction Accuracy:\",int(tlc.accuracy_score(y_test, y_pred)*1000)/10,'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjDX6w5ByYd4"
      },
      "source": [
        "A more granular method is to view what is quite rightly called a **Confusion Matrix**.\n",
        "\n",
        "A confusion matrix is a grid mapping 'correct' answers to predictions. The rows represent the class assigned in the test data and the columns represent predictions. The top left to bottom right diagonal shows us how many of each class have been predicted correctly. All of the other numbers count incorrect predictions. The number in row 2, column 3, will show how many documents of whatever the 2nd class represents (\"Medicine\") have been misclassified as the 3rd class (\"Rail\").\n",
        "\n",
        "In this example we see that \"Rail\" documents tend to be classified correctly, but a lot of the other types are being also misclassified as \"Rail\".\n",
        "\n",
        "We can see from the numbers that this dataset is highly imbalanced, so most of the records are \"Rail\". This may be responsible for the bias towards that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6HOTYBzylPl"
      },
      "source": [
        "fig, ax = tlc.draw_confusion(y_test, y_pred, model, class_names)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esv7JOuSQUuM"
      },
      "source": [
        "### Exploring individual predictions\n",
        "Similar to the topic model output, the prediction also comes with probabilities. We can visualise these probabilites for a selection of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rewyDFuGe6tk"
      },
      "source": [
        "y_true_pred = [x for x in zip(range(len(y_test)),y_test, y_pred, y_prob)]\n",
        "incorrect_predictions = [(y[0],y[1],y[2],y[3]) for i,y in enumerate(y_true_pred) if y[1] != y[2]]\n",
        "incorrect_unsure = [x for x in incorrect_predictions if max(x[3]) < 0.8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_1RTC4BfkQA"
      },
      "source": [
        "prediction_sample = tlc.random.sample(range(len(incorrect_unsure)), min(5, len(incorrect_unsure)))\n",
        "fig, ax = tlc.pyplot.subplots(min(5,len(prediction_sample)),1)\n",
        "fig.set_size_inches(5,7)\n",
        "\n",
        "for i, sample_idx in enumerate(prediction_sample):\n",
        "    prediction_row = incorrect_unsure[sample_idx]\n",
        "    data_idx = prediction_row[0]\n",
        "    true_class = prediction_row[1]\n",
        "    predicted = prediction_row[2]\n",
        "    tp = prediction_row[3]\n",
        "    class_probs = [tp[0],tp[1],tp[2],tp[3],tp[4]]\n",
        "    ax[i].set_ylim([0,1.0])\n",
        "    ax[i].text(.4,0.8, str(data_idx) + \": True:\" + class_names[true_class] + \", Predicted:\" + class_names[predicted],\n",
        "        horizontalalignment='center',\n",
        "        transform=ax[i].transAxes)\n",
        "    if i < 4:\n",
        "        ax[i].set_xticks([])\n",
        "    else:\n",
        "        ax[i].set_xticks(ticks=[0,1,2,3,4])\n",
        "        ax[i].set_xticklabels(labels=['General','Medicine','Rail', 'Safety', 'Pension'])\n",
        "    ax[i].bar(x = [0,1,2,3,4], height = class_probs)\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vxULPU9Tif-"
      },
      "source": [
        "If we look at probabilities in aggregate we see that generally high confidence predictions match the correct class, and lower confidence ones are more likely to be incorrect. This is desirable behaviour because it gives us more trust in the high confidence predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3kyDT8q0YET"
      },
      "source": [
        "\n",
        "prob_match_sum = tlc.np.zeros((11,4))\n",
        "\n",
        "for i in range(11):\n",
        "    prob_match_sum[i,0] = i/10\n",
        "for row in y_true_pred:\n",
        "    max_prob = int(max(row[3]) * 10)\n",
        "    prob_match_sum[max_prob,int(row[1] == row[2])+1] += 1\n",
        "    prob_match_sum[max_prob,3] += 1\n",
        "\n",
        "prob_match_sum = tlc.pd.DataFrame(prob_match_sum, columns=[\"Probability\", \"NoMatch\", \"Match\", \"Total\"])\n",
        "\n",
        "ax = prob_match_sum.plot(x=\"Probability\", y=\"Total\", kind=\"bar\", color=\"blue\")\n",
        "ax.legend(['Disagree', 'Match'])\n",
        "prob_match_sum.plot(x=\"Probability\", y=\"Match\", kind=\"bar\", ax=ax, color=\"orange\")\n",
        "tlc.pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CaWTrbrTHkv"
      },
      "source": [
        "We will now use another Machine Learning algorithm called Nearest Neighbours to help use classify some new training data.\n",
        "\n",
        "This code prepares the data for the next section. Firstly the data is indexed to find similar documents quickly (KDTree), then we sort the predictions by their prediction probability (most confident first). Finally we create a sample of most and least confident predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ksyQlzLvHMu"
      },
      "source": [
        "kdt = tlc.KDTree(training_features, leaf_size=30, metric='minkowski', p=2)\n",
        "tfidf_words = D4ML.vectorizer.get_feature_names()\n",
        "max_sorted_asc = sorted(y_true_pred, key=lambda max_x : max(max_x[3]), reverse=True)\n",
        "sample_ids = (tlc.np.random.beta(0.55, 0.4, 50) * len(max_sorted_asc)).astype('int')\n",
        "sample_ids = list(set(sample_ids))\n",
        "#tlc.random.shuffle(sample_ids)\n",
        "s = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_JZ48ucpHfP"
      },
      "source": [
        "# Reviewing and correcting predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXNNteGgf-cA"
      },
      "source": [
        "## Form setup code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0kwWTXEBnAN"
      },
      "source": [
        "output = tlc.widgets.Output()\n",
        "\n",
        "neighbour_list = []\n",
        "neighbour_idx = 0\n",
        "NUMBER_OF_NEIGHBOURS = 4\n",
        "\n",
        "human_classification = {}\n",
        "\n",
        "def dropdown_update(change):\n",
        "\n",
        "    check_row = change.new\n",
        "    this_idx = max_sorted_asc[check_row][0]\n",
        "    this_words = set([tfidf_words[w] for w in tlc.np.nonzero(X_test[this_idx])[1]])\n",
        "    true_class_drop.value = y_test[this_idx]\n",
        "    this_file_name = D4ML.idx_to_file[x_test_ids[this_idx]]\n",
        "    file_name_text.set_state({'value':this_file_name})\n",
        "    file_name_text.send_state('value')\n",
        "    predicted_class_text.set_state({'value': class_names[y_pred[this_idx]]})\n",
        "    predicted_class_text.send_state('value')\n",
        "    prediction_prob = tlc.np.max(y_prob[this_idx])\n",
        "    doc_pred_prob.set_state({'value': prediction_prob})\n",
        "    doc_pred_prob.send_state('value')\n",
        "    doc_words.set_state({'value': \",\".join(list(this_words))})\n",
        "    doc_words.send_state('value')\n",
        "    nn_dist, nn_ind = kdt.query(X_test[this_idx], k=NUMBER_OF_NEIGHBOURS)\n",
        "    doc_contents.set_state({'value': D4ML.file_contents[this_file_name]})\n",
        "    doc_contents.send_state('value')\n",
        "    neighbour_count = 0\n",
        "    global neighbour_list\n",
        "    neighbour_list = []\n",
        "    for i in range(len(nn_dist[0])):\n",
        "        dist = nn_dist[0][i]\n",
        "        idx =  nn_ind[0][i]\n",
        "        file_name = D4ML.idx_to_file[idx]\n",
        "        if idx == x_test_ids[this_idx]:\n",
        "            continue\n",
        "        pred = model.predict(training_features[idx])\n",
        "        neighbour_distance.set_state({'value':dist})\n",
        "        neighbour_distance.send_state('value')\n",
        "        true_class = -1\n",
        "        words = set([tfidf_words[w] for w in tlc.np.nonzero(training_features[idx])[1]])\n",
        "        if len(words) == 0:\n",
        "            continue\n",
        "        if file_name in class_probs:\n",
        "            true_class = tlc.np.argmax(class_probs[file_list[idx]])\n",
        "        overlap_words = \",\".join(list(words.intersection(this_words)))\n",
        "        \n",
        "        neighbour_list.append([idx, dist, file_name, overlap_words, D4ML.file_contents[file_name],\n",
        "                               true_class, class_names[pred[0]]])\n",
        "        neighbour_count += 1\n",
        "    accordion.set_title(3, \"Nearest Neighbours: \" + str(len(neighbour_list)))\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        display(accordion)\n",
        "\n",
        "\n",
        "sample_dropdown = tlc.widgets.Dropdown(options=sample_ids, value=None, description='Choose a document',\n",
        "                                       layout={'width':'100px'}, style={'description_width': 'initial'})\n",
        "sample_dropdown.observe(dropdown_update, 'value')\n",
        "\n",
        "file_name_text = tlc.widgets.Text(value=None, description=\"File Name\")\n",
        "class_options = [(v,k) for k,v in class_names.items()]\n",
        "true_class_drop = tlc.widgets.Dropdown(options=class_options,\n",
        "                                       value = None, description='True Class')\n",
        "update_true = tlc.widgets.Button(description=\"Update\")\n",
        "\n",
        "def on_true_pressed(b):\n",
        "    human_classification[file_name_text.value] = true_class_drop.value\n",
        "\n",
        "update_true.on_click(on_true_pressed)\n",
        "\n",
        "#true_class_drop.observe(on_true_changed,'value')\n",
        "predicted_class_text = tlc.widgets.Text(value=None, description='Predicted class', style={'description_width': 'initial'})\n",
        "doc_pred_prob = tlc.widgets.FloatText(value=None, description='Probability')\n",
        "details = tlc.widgets.VBox([file_name_text,\n",
        "                            tlc.widgets.HBox([true_class_drop, predicted_class_text, update_true]),\n",
        "                            doc_pred_prob])\n",
        "\n",
        "neighbour_overlap = tlc.widgets.Text(value=\"\", \n",
        "                               layout={'height': '100%', 'width': '700px'}, disabled=False)\n",
        "neighbour_distance = tlc.widgets.FloatText(value=None, description='Distance')\n",
        "neighbour_true_class = tlc.widgets.Dropdown(value=None, description=\"True\", options=class_options)\n",
        "neighbour_prediction = tlc.widgets.Text(value=None, description=\"Prediction\")\n",
        "neighbour_file = tlc.widgets.Text(value=None)\n",
        "neighbour_content = tlc.widgets.Textarea(value=None, layout={'height': '150px', 'width': '700px'})\n",
        "\n",
        "def on_next_clicked(b):\n",
        "    global neighbour_idx\n",
        "    global neighbour_list\n",
        "    if len(neighbour_list) == 0:\n",
        "        return\n",
        "    neighbour_idx += 1\n",
        "    if neighbour_idx >= len(neighbour_list):\n",
        "        neighbour_idx = 0\n",
        "    neighbour_data = neighbour_list[neighbour_idx]\n",
        "    neighbour_true_class.value = neighbour_data[5]\n",
        "    neighbour_true_class.send_state('value')\n",
        "    neighbour_prediction.set_state({'value':neighbour_data[6]})\n",
        "    neighbour_prediction.send_state('value')\n",
        "    neighbour_distance.set_state({'value':neighbour_data[1]})\n",
        "    neighbour_distance.send_state('value')\n",
        "    neighbour_file.set_state({'value':neighbour_data[2]})\n",
        "    neighbour_file.send_state('value')\n",
        "    neighbour_overlap.set_state({'value':neighbour_data[3]})\n",
        "    neighbour_overlap.send_state('value')\n",
        "    neighbour_content.set_state({'value':neighbour_data[4]})\n",
        "    neighbour_content.send_state('value')\n",
        "\n",
        "\n",
        "next_neighbour = tlc.widgets.Button(description=\"Next\", layout={'width':'100px'})\n",
        "next_neighbour.on_click(on_next_clicked)\n",
        "\n",
        "neighbour_true_update = tlc.widgets.Button(description=\"Update\")\n",
        "\n",
        "def on_neighbour_update_click(b):\n",
        "    human_classification[neighbour_file.value] = neighbour_true_class.value\n",
        "\n",
        "neighbour_true_update.on_click(on_neighbour_update_click)\n",
        "\n",
        "neighbour_details = tlc.widgets.VBox([next_neighbour, neighbour_file, neighbour_distance,\n",
        "                                      tlc.widgets.HBox([neighbour_true_class, neighbour_true_update]),\n",
        "                                      neighbour_prediction])\n",
        "\n",
        "doc_words = tlc.widgets.Text(value=None, layout={'height': '100%', 'width': '700px'}, disabled=False)\n",
        "#overlap.observe(sample_dropdown, on_button_clicked)\n",
        "doc_contents = tlc.widgets.Textarea(value=None,\n",
        "                                    layout={'height': '200px', 'width': '700px'})\n",
        "sub_accordion = tlc.widgets.Accordion(children=[neighbour_details, neighbour_overlap, neighbour_content])\n",
        "accordion = tlc.widgets.Accordion(children=[details,doc_words, doc_contents, sub_accordion])\n",
        "accordion.set_title(0,\"Details\")\n",
        "accordion.set_title(1,\"Document Features\")\n",
        "accordion.set_title(2,\"Document Contents\")\n",
        "accordion.set_title(3,\"Nearest Neighbours\")\n",
        "sub_accordion.set_title(0,\"Neighbour details\")\n",
        "sub_accordion.set_title(1,\"Word overlap\")\n",
        "sub_accordion.set_title(2,\"Neighbour content\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy0Y8iEU1oFS"
      },
      "source": [
        "## Refreshing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCdenh_J22Ud"
      },
      "source": [
        "Run the line of code below and then use the dropdown menu to select a sample document. The ids will be meaningless so pick any. This will open a form for viewing the classification results for the document. The form layout is called 'accordion' and you can click on any title and a different part of the form will open up. You can also change the 'true' classification and press update to save the new value. As well as checking the selected document classification, the form also shows the most similar documents (using nearest neighbours) to classify those at the same time.\n",
        "\n",
        "The output from this exercise could be used in future to train a new classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh0fHMi5KTGa"
      },
      "source": [
        "display(sample_dropdown, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4geniw2NGodR"
      },
      "source": [
        "human_classification"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}